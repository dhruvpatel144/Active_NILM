{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 00:46:27.063984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from dataloader import *\n",
    "from model import *\n",
    "from utilities import *\n",
    "from retrain_fun import *\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "jax.random.PRNGKey(0)\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_reductions --xla_gpu_autotune_level=2\"\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aggregated, houses = data_preprocess(only_positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3538, 5746, 7719, 7901, 8565, 9278, 8156, 8386, 9160, 9019,  661,\n",
       "       1642, 2335, 2361, 2818, 3456, 4373, 7536, 7800, 7951])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = [[2361, 7719, 9019, 2335, 7951, 5746, 8565, 9278, 8156, 8386, 9160, 661, 1642, 7536, 7800]]\n",
    "test_list = [[4373, 7901, 3456, 3538, 2818]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_bs_lr = {\n",
    "    \"air\": (1, 0.0001),\n",
    "    \"refrigerator\": (4096, 0.01),\n",
    "    \"furnace\": (2048, 0.0001),\n",
    "    \"dishwasher\": (4096, 0.0001),\n",
    "    \"clotheswasher\": (2048, 0.0001),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "appliance = \"air\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_houses = train_list[0]\n",
    "test_houses = test_list[0]\n",
    "train = data_aggregated[data_aggregated[\"dataid\"].isin(train_houses)]\n",
    "test = data_aggregated[data_aggregated[\"dataid\"].isin(test_houses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 00:47:20.387457: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc:825] None of the algorithms provided by cuDNN heuristics worked; trying fallback algorithms.  Conv: (f32[1030660,40,99]{2,1,0}, u8[0]{0}) custom-call(f32[1030660,30,104]{2,1,0}, f32[40,30,6]{2,1,0}), window={size=6}, dim_labels=bf0_oi0->bf0, custom_call_target=\"__cudnn$convForward\", backend_config=\"{\\\"conv_result_scale\\\":1,\\\"activation_mode\\\":\\\"0\\\",\\\"side_input_scale\\\":0}\"\n",
      "2023-05-08 00:47:32.227135: W external/org_tensorflow/tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 19.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "UNKNOWN: Failed to determine best cudnn convolution algorithm for:\n%cudnn-conv.1 = (f32[1030660,50,99]{2,1,0}, u8[0]{0}) custom-call(f32[1030660,50,99]{2,1,0} %transpose, f32[50,50,5]{2,1,0} %transpose.1), window={size=5 pad=2_2}, dim_labels=bf0_oi0->bf0, custom_call_target=\"__cudnn$convForward\", metadata={op_name=\"jit(conv_general_dilated)/jit(main)/conv_general_dilated[window_strides=(1,) padding=((2, 2),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/home/dhruv.patel/final_active/pos/model.py\" source_line=25}, backend_config=\"{\\\"conv_result_scale\\\":1,\\\"activation_mode\\\":\\\"0\\\",\\\"side_input_scale\\\":0}\"\n\nOriginal error: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 20423845216 bytes.\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m y_train \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray(y_train)\n\u001b[1;32m      9\u001b[0m model \u001b[39m=\u001b[39m seq2point()\n\u001b[0;32m---> 10\u001b[0m params \u001b[39m=\u001b[39m  model\u001b[39m.\u001b[39;49minit(jax\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mPRNGKey(\u001b[39m0\u001b[39;49m), x_train, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     11\u001b[0m params, losses \u001b[39m=\u001b[39m fit(model, params, x_train, y_train,\u001b[39mFalse\u001b[39;00m, batch_size\u001b[39m=\u001b[39mdict_bs_lr[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mappliance\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m], learning_rate\u001b[39m=\u001b[39mdict_bs_lr[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mappliance\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m], epochs\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[1;32m     12\u001b[0m x_test, y_test \u001b[39m=\u001b[39m dataloader(appliance, test, \u001b[39m\"\u001b[39m\u001b[39m2018-05-01 00:00:00-06\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m2018-05-10 23:59:00-06\u001b[39m\u001b[39m\"\u001b[39m,n)\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m~/final_active/pos/model.py:25\u001b[0m, in \u001b[0;36mseq2point.__call__\u001b[0;34m(self, X, deterministic)\u001b[0m\n\u001b[1;32m     23\u001b[0m X \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mrelu(X)\n\u001b[1;32m     24\u001b[0m X \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(rate\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, deterministic\u001b[39m=\u001b[39mdeterministic)(X)\n\u001b[0;32m---> 25\u001b[0m X \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mConv(\u001b[39m50\u001b[39;49m, kernel_size\u001b[39m=\u001b[39;49m(\u001b[39m5\u001b[39;49m,))(X)\n\u001b[1;32m     26\u001b[0m X \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mrelu(X)\n\u001b[1;32m     27\u001b[0m X \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(rate\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, deterministic\u001b[39m=\u001b[39mdeterministic)(X)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/active_NILM/lib/python3.9/site-packages/flax/linen/linear.py:439\u001b[0m, in \u001b[0;36m_Conv.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    437\u001b[0m inputs, kernel, bias \u001b[39m=\u001b[39m promote_dtype(inputs, kernel, bias, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshared_weights:\n\u001b[0;32m--> 439\u001b[0m   y \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39;49mconv_general_dilated(\n\u001b[1;32m    440\u001b[0m       inputs,\n\u001b[1;32m    441\u001b[0m       kernel,\n\u001b[1;32m    442\u001b[0m       strides,\n\u001b[1;32m    443\u001b[0m       padding_lax,\n\u001b[1;32m    444\u001b[0m       lhs_dilation\u001b[39m=\u001b[39;49minput_dilation,\n\u001b[1;32m    445\u001b[0m       rhs_dilation\u001b[39m=\u001b[39;49mkernel_dilation,\n\u001b[1;32m    446\u001b[0m       dimension_numbers\u001b[39m=\u001b[39;49mdimension_numbers,\n\u001b[1;32m    447\u001b[0m       feature_group_count\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_group_count,\n\u001b[1;32m    448\u001b[0m       precision\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision\n\u001b[1;32m    449\u001b[0m   )\n\u001b[1;32m    450\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m   y \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mconv_general_dilated_local(\n\u001b[1;32m    452\u001b[0m       lhs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    453\u001b[0m       rhs\u001b[39m=\u001b[39mkernel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    460\u001b[0m       precision\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision\n\u001b[1;32m    461\u001b[0m   )\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/active_NILM/lib/python3.9/site-packages/jax/_src/dispatch.py:1026\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, built_c, options, host_callbacks)\u001b[0m\n\u001b[1;32m   1021\u001b[0m   \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mcompile(built_c, compile_options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m   1022\u001b[0m                          host_callbacks\u001b[39m=\u001b[39mhost_callbacks)\n\u001b[1;32m   1023\u001b[0m \u001b[39m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[39m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[39m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mcompile(built_c, compile_options\u001b[39m=\u001b[39;49moptions)\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: UNKNOWN: Failed to determine best cudnn convolution algorithm for:\n%cudnn-conv.1 = (f32[1030660,50,99]{2,1,0}, u8[0]{0}) custom-call(f32[1030660,50,99]{2,1,0} %transpose, f32[50,50,5]{2,1,0} %transpose.1), window={size=5 pad=2_2}, dim_labels=bf0_oi0->bf0, custom_call_target=\"__cudnn$convForward\", metadata={op_name=\"jit(conv_general_dilated)/jit(main)/conv_general_dilated[window_strides=(1,) padding=((2, 2),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/home/dhruv.patel/final_active/pos/model.py\" source_line=25}, backend_config=\"{\\\"conv_result_scale\\\":1,\\\"activation_mode\\\":\\\"0\\\",\\\"side_input_scale\\\":0}\"\n\nOriginal error: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 20423845216 bytes.\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning."
     ]
    }
   ],
   "source": [
    "n = 99\n",
    "x_train, y_train = dataloader(appliance, train, \"2018-03-01 00:00:00-06\", \"2018-04-30 23:59:00-06\", n)\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "x_train = scaler_x.fit_transform(x_train)\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "x_train = jnp.array(x_train).reshape(x_train.shape[0], n, 1)\n",
    "y_train = jnp.array(y_train)\n",
    "model = seq2point()\n",
    "params = model.init(jax.random.PRNGKey(0), x_train, True)\n",
    "params, losses = fit(\n",
    "    model,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    False,\n",
    "    batch_size=dict_bs_lr[f\"{appliance}\"][0],\n",
    "    learning_rate=dict_bs_lr[f\"{appliance}\"][1],\n",
    "    epochs=30,\n",
    ")\n",
    "x_test, y_test = dataloader(appliance, test, \"2018-05-01 00:00:00-06\", \"2018-05-10 23:59:00-06\", n)\n",
    "x_test = scaler_x.transform(x_test)\n",
    "x_test = jnp.array(x_test).reshape(x_test.shape[0], n, 1)\n",
    "y_test = jnp.array(y_test)\n",
    "y_hat = model.apply(params, x_test, True, rngs={\"dropout\": jax.random.PRNGKey(0)})\n",
    "test_mean = scaler_y.inverse_transform(y_hat[0])\n",
    "test_sigma = scaler_y.scale_ * y_hat[1]\n",
    "print(f\"RMSE : {rmse(y_test, test_mean):.4f}, MAE : {mae(y_test, test_mean):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.0001)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_bs_lr[\"air\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = 0\n",
    "idx2 = -1\n",
    "fig, ax = plt.subplots(2, 2, figsize=(18, 10))\n",
    "ax = ax.ravel()\n",
    "ax[0].plot(y_test[idx1:idx2], label=\"True\")\n",
    "ax[1].plot(test_mean[idx1:idx2], label=f\"$\\mu$ Predicted\", color=\"orange\")\n",
    "ax[2].plot(y_test[idx1:idx2], label=\"True\")\n",
    "ax[2].plot(test_mean[idx1:idx2], label=f\"$\\mu$ Predicted\", color=\"orange\")\n",
    "ax[3].plot(y_test[idx1:idx2], label=\"True\", alpha=0.7)\n",
    "ax[3].plot(test_sigma[idx1:idx2], label=f\"$\\sigma$ Predicted\", color=\"green\")\n",
    "ax[0].legend(fontsize=15, bbox_to_anchor=(0.5, 1))\n",
    "ax[1].legend(fontsize=15, bbox_to_anchor=(0.5, 1))\n",
    "ax[2].legend(fontsize=15, bbox_to_anchor=(0.5, 1))\n",
    "ax[3].legend(fontsize=15, bbox_to_anchor=(0.5, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "active_NILM",
   "language": "python",
   "name": "active_nilm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
